# Site2MD ‚Äî Website to Markdown CLI

> Crawl b·∫•t k·ª≥ trang web n√†o ‚Üí Markdown/JSONL/TXT t·ªëi ∆∞u cho **NotebookLM**, **RAG**, v√† c√°c h·ªá th·ªëng AI kh√°c.

---

## M·ª•c l·ª•c

- [T√≠nh nƒÉng](#t√≠nh-nƒÉng)
- [Y√™u c·∫ßu](#y√™u-c·∫ßu)
- [C√†i ƒë·∫∑t](#c√†i-ƒë·∫∑t)
- [C·∫•u h√¨nh](#c·∫•u-h√¨nh)
- [S·ª≠ d·ª•ng c∆° b·∫£n](#s·ª≠-d·ª•ng-c∆°-b·∫£n)
- [T·∫•t c·∫£ t√πy ch·ªçn](#t·∫•t-c·∫£-t√πy-ch·ªçn)
- [Output formats](#output-formats)
- [T√≠nh nƒÉng n√¢ng cao](#t√≠nh-nƒÉng-n√¢ng-cao)
  - [Crawl qua sitemap](#crawl-qua-sitemap)
  - [Crawl ƒë·ªá quy](#crawl-ƒë·ªá-quy)
  - [Fallback urls.txt](#fallback-urlstxt)
  - [CSS Selector](#css-selector)
  - [File splitting](#file-splitting)
  - [HTTP Caching](#http-caching)
  - [AI Refinement](#ai-refinement)
- [V√≠ d·ª• th·ª±c t·∫ø](#v√≠-d·ª•-th·ª±c-t·∫ø)
- [C·∫•u tr√∫c d·ª± √°n](#c·∫•u-tr√∫c-d·ª±-√°n)
- [X·ª≠ l√Ω l·ªói](#x·ª≠-l√Ω-l·ªói)
- [NotebookLM Tips](#notebooklm-tips)

---

## T√≠nh nƒÉng

| T√≠nh nƒÉng | M√¥ t·∫£ |
|---|---|
| **Sitemap crawler** | T·ª± ƒë·ªông t√¨m `sitemap.xml` qua `robots.txt` ho·∫∑c ƒë∆∞·ªùng d·∫´n m·∫∑c ƒë·ªãnh |
| **Recursive crawl** | Crawl ƒë·ªá quy theo ƒë·ªô s√¢u v·ªõi `--depth` khi kh√¥ng c√≥ sitemap |
| **Multi-format** | Xu·∫•t ra Markdown (`.md`), plain text (`.txt`), JSON Lines (`.jsonl`) |
| **File splitting** | T·ª± ƒë·ªông chia file khi v∆∞·ª£t gi·ªõi h·∫°n k√Ω t·ª± (t·ªëi ∆∞u cho NotebookLM 500k) |
| **YAML front-matter** | M·ªói trang ƒë∆∞·ª£c g·∫Øn metadata `source_url`, `title`, `collected_at` |
| **Table of Contents** | T·ª± ƒë·ªông t·∫°o m·ª•c l·ª•c li√™n k·∫øt cho to√†n b·ªô t√†i li·ªáu |
| **HTTP caching** | Cache HTTP 24h ƒë·ªÉ tr√°nh crawl l·∫°i (`--cache`) |
| **Concurrency** | Crawl song song nhi·ªÅu trang c√πng l√∫c (`--concurrency`) |
| **CSS Selector** | Tr√≠ch xu·∫•t ƒë√∫ng v√πng n·ªôi dung qua CSS selector (`--selector`) |
| **AI Cleaning** | D√πng Deepseek AI ƒë·ªÉ chu·∫©n h√≥a Markdown (`--ai-clean`) |
| **AI Summary** | T·ª± ƒë·ªông t√≥m t·∫Øt t·ª´ng trang b·∫±ng ti·∫øng Vi·ªát (`--ai-summary`) |
| **Dry run** | Preview danh s√°ch URLs v√† ∆∞·ªõc t√≠nh k·∫øt qu·∫£ (`--dry-run`) |
| **Error handling** | T·ª± retry (3 l·∫ßn, exponential backoff), log l·ªói v√†o `error.log` |

---

## Y√™u c·∫ßu

- **Python 3.10+**
- K·∫øt n·ªëi internet

---

## C√†i ƒë·∫∑t

### 1. Clone ho·∫∑c t·∫£i v·ªÅ d·ª± √°n

```bash
git clone https://github.com/yourname/site2md.git
cd site2md
```

### 2. C√†i ƒë·∫∑t dependencies

```bash
pip install -r requirements.txt
```

Ho·∫∑c n·∫øu pip kh√¥ng nh·∫≠n di·ªán:

```bash
python -m pip install -r requirements.txt
```

### 3. C·∫•u h√¨nh API (t√πy ch·ªçn ‚Äî ch·ªâ c·∫ßn cho AI features)

```bash
cp .env.example .env
# Ch·ªânh s·ª≠a .env v√† ƒëi·ªÅn DEEPSEEK_API_KEY n·∫øu mu·ªën d√πng --ai-clean / --ai-summary
```

---

## C·∫•u h√¨nh

### `.env`

```env
# Ch·ªâ c·∫ßn cho --ai-clean v√† --ai-summary
DEEPSEEK_API_KEY=sk-xxxxxxxxxxxxxxxxxxxx
DEEPSEEK_BASE_URL=https://api.deepseek.com  # M·∫∑c ƒë·ªãnh, c√≥ th·ªÉ b·ªè qua
```

### `config.py`

C√°c h·∫±ng s·ªë to√†n c·ª•c c√≥ th·ªÉ ƒëi·ªÅu ch·ªânh:

| H·∫±ng s·ªë | M·∫∑c ƒë·ªãnh | M√¥ t·∫£ |
|---|---|---|
| `DEFAULT_SPLIT_LIMIT` | `450_000` | Gi·ªõi h·∫°n k√Ω t·ª± m·ªói file |
| `REQUEST_TIMEOUT` | `30` gi√¢y | Timeout HTTP request |
| `MAX_RETRIES` | `3` | S·ªë l·∫ßn retry khi l·ªói m·∫°ng |
| `CACHE_TTL` | `86400` gi√¢y | Th·ªùi gian s·ªëng c·ªßa cache (24h) |
| `AI_MAX_CHUNK` | `12_000` | Max token m·ªói l·∫ßn g·ªçi AI |

---

## S·ª≠ d·ª•ng c∆° b·∫£n

```bash
# Crawl to√†n b·ªô site qua sitemap, xu·∫•t ra output.md
python main.py https://docs.example.com

# Crawl ƒë·ªá quy depth=2, ƒë·∫∑t t√™n output l√† "laravel_docs"
python main.py https://laravel.com/docs -o laravel_docs --depth 2

# Xu·∫•t JSONL (cho RAG vector store)
python main.py https://docs.example.com --format jsonl

# Xem tr∆∞·ªõc URLs tr∆∞·ªõc khi crawl
python main.py https://docs.example.com --dry-run
```

---

## T·∫•t c·∫£ t√πy ch·ªçn

```
Usage: python main.py [URL] [OPTIONS]

Arguments:
  URL  [required]  URL g·ªëc c·∫ßn crawl (v√≠ d·ª•: https://docs.example.com/)

Options:
  -o, --output    TEXT     T√™n file ƒë·∫ßu ra (kh√¥ng extension) [default: output]
  -f, --format    TEXT     Format: md | txt | jsonl          [default: md]
  -c, --concurrency INT   S·ªë request song song               [default: 5]
      --cache              B·∫≠t HTTP cache 24h
      --split-limit INT    Gi·ªõi h·∫°n k√Ω t·ª± m·ªói file          [default: 450000]
      --selector  TEXT     CSS selector v√πng n·ªôi dung
      --depth     INT      ƒê·ªô s√¢u crawl ƒë·ªá quy (0 = d√πng sitemap)
      --ai-clean           D√πng AI chu·∫©n h√≥a Markdown
      --ai-summary         D√πng AI t·∫°o t√≥m t·∫Øt ti·∫øng Vi·ªát
      --dry-run            Preview URLs kh√¥ng crawl
      --help               Hi·ªÉn th·ªã tr·ª£ gi√∫p
```

---

## Output formats

### `--format md` (m·∫∑c ƒë·ªãnh)

Ph√π h·ª£p nh·∫•t cho **NotebookLM** v√† ƒë·ªçc b·ªüi ng∆∞·ªùi.

```markdown
# üìë M·ª§C L·ª§C

T·ªïng s·ªë trang: **42**

1. [Getting Started](https://docs.example.com/getting-started)
2. [Installation](https://docs.example.com/installation)
...

---
---
source_url: https://docs.example.com/getting-started
title: "Getting Started"
collected_at: 2026-02-23T17:00:00+07:00
---

# Getting Started

N·ªôi dung trang...
```

### `--format jsonl`

Ph√π h·ª£p cho **RAG vector stores**, **LangChain**, **LlamaIndex**.

M·ªói d√≤ng l√† m·ªôt JSON object:

```json
{"url": "https://docs.example.com/page", "title": "Page Title", "content": "# Heading\n\nContent...", "collected_at": "2026-02-23T17:00:00+07:00"}
```

### `--format txt`

Xu·∫•t plain text ƒë∆°n gi·∫£n, kh√¥ng c√≥ Markdown formatting.

```
SITE2MD ‚Äî https://docs.example.com
Thu th·∫≠p: 2026-02-23T17:00:00+07:00
============================================================

[Getting Started] https://docs.example.com/getting-started
N·ªôi dung trang...
```

---

## T√≠nh nƒÉng n√¢ng cao

### Crawl qua sitemap

M·∫∑c ƒë·ªãnh, Site2MD t·ª± t√¨m sitemap theo th·ª© t·ª±:

1. ƒê·ªçc `robots.txt` ƒë·ªÉ t√¨m `Sitemap:` directive
2. Th·ª≠ `/sitemap.xml`
3. Th·ª≠ `/sitemap_index.xml`
4. Fallback sang `urls.txt` n·∫øu kh√¥ng t√¨m th·∫•y

```bash
python main.py https://docs.example.com
```

### Crawl ƒë·ªá quy

D√πng khi site kh√¥ng c√≥ sitemap, ho·∫∑c mu·ªën gi·ªõi h·∫°n s·ªë trang:

```bash
# Crawl t·ªëi ƒëa depth=1 (ch·ªâ c√°c link tr·ª±c ti·∫øp t·ª´ trang g·ªëc)
python main.py https://docs.example.com --depth 1

# Crawl s√¢u h∆°n
python main.py https://docs.example.com --depth 3
```

> **L∆∞u √Ω:** `--depth` ch·ªâ theo d√µi link n·ªôi b·ªô (c√πng domain).

### Fallback `urls.txt`

N·∫øu kh√¥ng t√¨m th·∫•y sitemap, t·∫°o file `urls.txt` trong th∆∞ m·ª•c ch·∫°y l·ªánh:

```
https://docs.example.com/page1
https://docs.example.com/page2
https://docs.example.com/page3
```

```bash
python main.py https://docs.example.com
# T·ª± ƒë·ªông ƒë·ªçc urls.txt khi sitemap kh√¥ng t√¨m th·∫•y
```

### CSS Selector

Khi trang c√≥ nhi·ªÅu UI noise (nav, footer, sidebar), d√πng `--selector` ƒë·ªÉ tr·ªè ƒë√∫ng v√πng n·ªôi dung:

```bash
# Ch·ªâ l·∫•y n·ªôi dung trong <article class="content">
python main.py https://docs.example.com --selector "article.content"

# L·∫•y main content
python main.py https://docs.example.com --selector "main"

# L·∫•y div c√≥ id c·ª• th·ªÉ
python main.py https://docs.example.com --selector "#page-content"
```

**C√°ch t√¨m selector:**
1. M·ªü DevTools trong tr√¨nh duy·ªát (F12)
2. Click v√†o v√πng n·ªôi dung ch√≠nh
3. Inspect element ‚Üí copy selector

### File splitting

NotebookLM gi·ªõi h·∫°n **500,000 k√Ω t·ª±** m·ªói file. Site2MD t·ª± ƒë·ªông chia file khi v∆∞·ª£t ng∆∞·ª°ng:

```
output.md        (450,000 chars)
output_part2.md  (ti·∫øp theo...)
output_part3.md  (n·∫øu c·∫ßn...)
```

T√πy ch·ªânh ng∆∞·ª°ng:

```bash
# Chia nh·ªè h∆°n (200k chars/file)
python main.py https://docs.example.com --split-limit 200000

# Kh√¥ng chia (cho Google Drive, etc.)
python main.py https://docs.example.com --split-limit 999999999
```

### HTTP Caching

B·∫≠t cache ƒë·ªÉ tr√°nh crawl l·∫°i trang ƒë√£ fetch, ti·∫øt ki·ªám th·ªùi gian khi ch·∫°y nhi·ªÅu l·∫ßn:

```bash
python main.py https://docs.example.com --cache
```

Cache ƒë∆∞·ª£c l∆∞u t·∫°i `.cache/` trong th∆∞ m·ª•c hi·ªán t·∫°i, TTL = 24h.

### AI Refinement

Y√™u c·∫ßu `DEEPSEEK_API_KEY` trong `.env`.

#### `--ai-clean` ‚Äî Chu·∫©n h√≥a Markdown

D√πng AI ƒë·ªÉ:
- S·ª≠a indentation code blocks
- Chu·∫©n h√≥a b·∫£ng Markdown
- Lo·∫°i b·ªè k√Ω t·ª± r√°c
- Th·ªëng nh·∫•t heading levels

```bash
python main.py https://docs.example.com --ai-clean
```

> X·ª≠ l√Ω th√™m ~1-2 gi√¢y/trang. C√≥ chunking t·ª± ƒë·ªông cho trang d√†i.

#### `--ai-summary` ‚Äî T√≥m t·∫Øt trang

Th√™m t√≥m t·∫Øt ~50 t·ª´ b·∫±ng ti·∫øng Vi·ªát v√†o ƒë·∫ßu m·ªói trang:

```bash
python main.py https://docs.example.com --ai-summary
```

#### K·∫øt h·ª£p c·∫£ hai

```bash
python main.py https://docs.example.com --ai-clean --ai-summary
```

---

## V√≠ d·ª• th·ª±c t·∫ø

### Scrape Python docs ƒë·ªÉ h·ªèi NotebookLM

```bash
python main.py https://docs.python.org/3/ --depth 1 -o python_docs --concurrency 10
```

‚Üí T·∫°o `python_docs.md` + `python_docs_part2.md`, upload c·∫£ 2 l√™n NotebookLM.

### Scrape docs framework cho RAG

```bash
python main.py https://fastapi.tiangolo.com --format jsonl -o fastapi_docs
```

‚Üí T·∫°o `fastapi_docs.jsonl`, import v√†o LangChain/LlamaIndex.

### Site c√≥ sidebar ph·ª©c t·∫°p

```bash
python main.py https://docs.rust-lang.org/book/ \
  --selector "div#content main" \
  --depth 2 \
  -o rust_book
```

### Crawl nhanh v·ªõi cache (l·∫ßn 2 tr·ªü ƒëi)

```bash
# L·∫ßn ƒë·∫ßu: crawl th·ª±c t·∫ø
python main.py https://docs.example.com --cache -o docs

# L·∫ßn sau: ƒë·ªçc t·ª´ cache (<1 gi√¢y)
python main.py https://docs.example.com --cache -o docs_v2
```

### Preview tr∆∞·ªõc khi crawl site l·ªõn

```bash
python main.py https://docs.example.com --dry-run
```

Output:

```
--------------------------------------------------
[DRY RUN] Preview (khong crawl thuc te)
--------------------------------------------------
  URL          : https://docs.example.com
  Tong URL     : 523
  Format       : md
  Split limit  : 450,000 chars
  Output       : output.md
  Est. size    : ~2,615,000 chars (6 part(s))
  Est. time    : ~157s
--------------------------------------------------

  URL list (first 10):
    - https://docs.example.com/
    - https://docs.example.com/guide/
    ...
```

---

## C·∫•u tr√∫c d·ª± √°n

```
Site2md CLI Tool/
‚îú‚îÄ‚îÄ main.py              # CLI entry point (Typer)
‚îú‚îÄ‚îÄ config.py            # H·∫±ng s·ªë v√† c·∫•u h√¨nh
‚îú‚îÄ‚îÄ requirements.txt     # Dependencies
‚îú‚îÄ‚îÄ .env.example         # Template bi·∫øn m√¥i tr∆∞·ªùng
‚îú‚îÄ‚îÄ .env                 # API keys (kh√¥ng commit git!)
‚îÇ
‚îú‚îÄ‚îÄ crawler/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ sitemap.py       # T√¨m sitemap, parse XML, crawl ƒë·ªá quy
‚îÇ   ‚îî‚îÄ‚îÄ fetcher.py       # Async HTTP client (httpx + hishel cache + retry)
‚îÇ
‚îú‚îÄ‚îÄ extractor/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ cleaner.py       # X√≥a UI noise (nav/footer/ads) b·∫±ng BeautifulSoup
‚îÇ   ‚îî‚îÄ‚îÄ content.py       # Tr√≠ch xu·∫•t n·ªôi dung (trafilatura ‚Üí markdownify fallback)
‚îÇ
‚îî‚îÄ‚îÄ formatter/
    ‚îú‚îÄ‚îÄ __init__.py
    ‚îú‚îÄ‚îÄ markdown.py      # Build YAML block, JSONL record, Table of Contents
    ‚îú‚îÄ‚îÄ splitter.py      # Chia file t·ª± ƒë·ªông theo gi·ªõi h·∫°n k√Ω t·ª±
    ‚îî‚îÄ‚îÄ ai_refiner.py    # Deepseek API integration (clean + summary)
```

### Lu·ªìng x·ª≠ l√Ω

```
URL Input
   ‚îÇ
   ‚îú‚îÄ‚îÄ sitemap.py ‚Üí T√¨m danh s√°ch URLs (sitemap / ƒë·ªá quy / urls.txt)
   ‚îÇ
   ‚îî‚îÄ‚îÄ fetcher.py ‚Üí Fetch HTML song song (async, cache, retry)
          ‚îÇ
          ‚îú‚îÄ‚îÄ cleaner.py ‚Üí X√≥a noise HTML, √°p d·ª•ng CSS selector
          ‚îÇ
          ‚îú‚îÄ‚îÄ content.py ‚Üí trafilatura ‚Üí markdown content
          ‚îÇ              ‚Üí markdownify (fallback)
          ‚îÇ
          ‚îú‚îÄ‚îÄ ai_refiner.py ‚Üí (t√πy ch·ªçn) AI clean + summary
          ‚îÇ
          ‚îî‚îÄ‚îÄ splitter.py ‚Üí Ghi ra file (md/txt/jsonl), t·ª± split
```

---

## X·ª≠ l√Ω l·ªói

### `error.log`

M·ªçi l·ªói crawl ƒë∆∞·ª£c ghi v√†o `error.log` trong th∆∞ m·ª•c hi·ªán t·∫°i:

```
2026-02-23 17:00:01 WARNING [SKIPPED] https://example.com/page - HTTP 403
2026-02-23 17:00:05 WARNING [SKIPPED] https://example.com/other - Timeout
```

### Retry t·ª± ƒë·ªông

- **3 l·∫ßn retry** v·ªõi exponential backoff (1s ‚Üí 2s ‚Üí 4s)
- √Åp d·ª•ng cho: timeout, connection error, HTTP 5xx
- HTTP 403, 404: skip ngay, kh√¥ng retry

### Trang kh√¥ng extract ƒë∆∞·ª£c

- N·∫øu `trafilatura` kh√¥ng extract ƒë∆∞·ª£c ‚Üí th·ª≠ `markdownify`
- N·∫øu c·∫£ hai th·∫•t b·∫°i ‚Üí ghi `[SKIPPED]` v√†o `error.log`, ti·∫øp t·ª•c trang kh√°c
- Trang b·ªã skip **kh√¥ng l√†m d·ª´ng** to√†n b·ªô qu√° tr√¨nh crawl

---

## NotebookLM Tips

### Upload nhi·ªÅu file

Khi site c√≥ nhi·ªÅu trang, Site2MD t·ª± chia th√†nh nhi·ªÅu file `_part1`, `_part2`... Upload t·∫•t c·∫£ l√™n NotebookLM c√πng m·ªôt l√∫c.

### Gi·ªõi h·∫°n 500k k√Ω t·ª±

NotebookLM t·ª´ ch·ªëi file > 500,000 k√Ω t·ª±. M·∫∑c ƒë·ªãnh Site2MD d√πng `--split-limit 450000` ƒë·ªÉ c√≥ buffer an to√†n.

### T·ªëi ∆∞u h√≥a ƒë·ªô li√™n quan

D√πng `--selector` ƒë·ªÉ ch·ªâ l·∫•y n·ªôi dung ch√≠nh, b·ªè qua nav/footer:

```bash
# K·∫øt qu·∫£ t·ªët h∆°n cho Q&A
python main.py https://docs.example.com --selector "main article"
```

### Format n√†o t·ªët nh·∫•t?

| Use case | Format |
|---|---|
| NotebookLM | `md` (m·∫∑c ƒë·ªãnh) |
| LangChain / LlamaIndex | `jsonl` |
| ƒê·ªçc th·ªß c√¥ng | `txt` |
| Chroma / Pinecone | `jsonl` |

---

## Dependencies ch√≠nh

| Package | M·ª•c ƒë√≠ch |
|---|---|
| `typer` | CLI framework |
| `httpx` | Async HTTP client |
| `hishel` | HTTP cache cho httpx |
| `beautifulsoup4` + `lxml` | HTML parsing & cleaning |
| `trafilatura` | Content extraction |
| `markdownify` | HTML ‚Üí Markdown fallback |
| `openai` | Deepseek API client (AI features) |
| `tqdm` | Progress bar |
| `python-dotenv` | ƒê·ªçc `.env` |

---

## License

MIT
